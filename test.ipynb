{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg(models:list):\n",
    "    return np.mean(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def krum(models:list):\n",
    "    return np.max(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(func, *data):\n",
    "    return func(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = fedavg\n",
    "aggregator(a, [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,]\n",
    "a.remove(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chao/git/beidou/venv/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.75"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "a = {1:2, 2:4, 3:34, 4:3, 5:23}\n",
    "X = np.array(list(a.values())).reshape(-1, 1)\n",
    "k_means = KMeans(n_clusters=1)\n",
    "k_means.fit(X)\n",
    "k_means_cluster_centers = k_means.cluster_centers_\n",
    "np.mean(k_means_cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [\"apple\", \"banana\", \"cherry\"]\n",
    "mylist= random.shuffle(mylist)\n",
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {0: 0.050472747534513474, 1: 0.33852526545524597, 2: 0.3366793096065521, 3: 0.322217732667923, 4: 0.3181234896183014, 5: 0.34658750891685486, 6: 0.30072730779647827, 7: 0.31256982684135437, 8: 0.3395855724811554, 9: 1.0}\n",
    "X = np.array(list(a.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bayesian_gmm(data, max_components=3):\n",
    "    bgmm = BayesianGaussianMixture(n_components=max_components, random_state=0)\n",
    "    bgmm.fit(data.reshape(-1, 1))\n",
    "    return bgmm\n",
    "\n",
    "def get_effective_components(bgmm, threshold=1e-3):\n",
    "    effective_components = np.sum(bgmm.weights_ > threshold)\n",
    "    return effective_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgmm = GaussianMixture(n_components=3, random_state=0)\n",
    "bgmm.fit_predict(X.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_gmm_components(data, max_components=5):\n",
    "    aic = []\n",
    "    bic = []\n",
    "    models = []\n",
    "    \n",
    "    for n in range(1, max_components + 1):\n",
    "        gmm = GaussianMixture(n_components=n, random_state=0)\n",
    "        gmm.fit(data.reshape(-1, 1))\n",
    "        aic.append(gmm.aic(data.reshape(-1, 1)))\n",
    "        bic.append(gmm.bic(data.reshape(-1, 1)))\n",
    "        models.append(gmm)\n",
    "    \n",
    "    return aic, bic, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic1, bic1, models1 = select_gmm_components(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.values()\n",
    "X = np.array(list(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 2, 4, 3, 0])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "mean_shift = MeanShift()\n",
    "mean_shift.fit_predict(X.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.326877  ],\n",
       "       [1.        ],\n",
       "       [0.05047275]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_shift.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.727203986385586,\n",
       "  -13.684793638169815,\n",
       "  -39.80753849865127,\n",
       "  -38.64296804532699,\n",
       "  -37.56682305383056],\n",
       " [3.3323741723736777,\n",
       "  -12.171868173199584,\n",
       "  -37.3868577546989,\n",
       "  -35.314532022392484,\n",
       "  -33.33063175191392])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aic1, bic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "opt = OPTICS()\n",
    "opt.fit_predict(X.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db=DBSCAN(eps=0.1, min_samples=1)\n",
    "clusters = db.fit_predict(X.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_points = X[clusters == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.1, min_samples=1)\n",
    "clusters = db.fit_predict(X.reshape(-1,1))\n",
    "unique_labels = set(clusters)\n",
    "\n",
    "if len(unique_labels) > 1:\n",
    "    trigger = True            \n",
    "\n",
    "    lowerbound = []\n",
    "    upperbound = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        cluster_points = X[clusters == label]\n",
    "        lowerbound.append(min(cluster_points))\n",
    "        upperbound.append(max(cluster_points))\n",
    "    \n",
    "    # the threshold is the average of lowerbound of the first class and upperbound of the second class\n",
    "    rep_threshold = np.mean([lowerbound[-1], upperbound[-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.050472747534513474, 0.30072730779647827, 1.0]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowerbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.050472747534513474, 0.34658750891685486, 1.0]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upperbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6732937544584274"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3395855724811554"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = list(b)\n",
    "c.sort()\n",
    "c[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33852527, 0.33667931, 0.32221773, 0.31812349, 0.34658751,\n",
       "       0.30072731, 0.31256983, 0.33958557])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3. ],\n",
       "       [28.5]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means_cluster_centers = k_means.cluster_centers_\n",
    "k_means_cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.33852526545524597, 0.3366793096065521, 0.322217732667923, 0.3181234896183014, 0.34658750891685486, 0.30072730779647827, 0.31256982684135437, 0.3395855724811554])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {1: 0.33852526545524597, 2: 0.3366793096065521, 3: 0.322217732667923, 4: 0.3181234896183014, 5: 0.34658750891685486, 6: 0.30072730779647827, 7: 0.31256982684135437, 8: 0.3395855724811554}\n",
    "a.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.666666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregator(fedavg, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregator(krum, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(1)==int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import generate_attack_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attacked_node_list_[8, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'attack_type': 'label flipping',\n",
       "  'targeted': False,\n",
       "  'noise_injected_ratio': 20,\n",
       "  'poisoned_sample_ratio': 100},\n",
       " 1: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 2: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 3: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 4: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 5: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 6: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 7: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 8: {'attack_type': 'label flipping',\n",
       "  'targeted': False,\n",
       "  'noise_injected_ratio': 20,\n",
       "  'poisoned_sample_ratio': 100},\n",
       " 9: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_list = list(range(10))\n",
    "attack_type = 'label flipping'\n",
    "targeted = False\n",
    "poisoned_node_ratio = 20\n",
    "noise_injected_ratio = 20\n",
    "poisoned_sample_ratio = 100\n",
    "generate_attack_matrix(node_list, attack_type, targeted, poisoned_node_ratio, noise_injected_ratio, poisoned_sample_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict, List\n",
    "import torch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_metric2(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        logging.info(\"Cosine similarity cannot be computed due to missing model\")\n",
    "        return None\n",
    "\n",
    "    cos_similarities = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "            if l1.shape != l2.shape:\n",
    "                # Adjust the shape of the smaller layer to match the larger layer\n",
    "                min_len = min(l1.shape[0], l2.shape[0])\n",
    "                l1, l2 = l1[:min_len], l2[:min_len]\n",
    "\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(l1.unsqueeze(0), l2.unsqueeze(0), dim=1)\n",
    "            cos_similarities.append(cos_sim.item())\n",
    "\n",
    "    if cos_similarities:\n",
    "        avg_cos_sim = torch.mean(torch.tensor(cos_similarities))\n",
    "        # result = torch.clamp(avg_cos_sim, min=0).item()\n",
    "        # return result\n",
    "        return avg_cos_sim.item() if similarity else (1 - avg_cos_sim.item())\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def cosine_metric(model1: OrderedDict, model2: OrderedDict, similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        logging.info(\"Cosine similarity cannot be computed due to missing model\")\n",
    "        return None\n",
    "\n",
    "    cos_similarities: List = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].to('cpu')\n",
    "            l2 = model2[layer].to('cpu')\n",
    "            if l1.shape != l2.shape:\n",
    "                # Adjust the shape of the smaller layer to match the larger layer\n",
    "                min_len = min(l1.shape[0], l2.shape[0])\n",
    "                l1, l2 = l1[:min_len], l2[:min_len]\n",
    "            cos = torch.nn.CosineSimilarity(dim=l1.dim() - 1)\n",
    "            cos_mean = torch.mean(cos(l1.float(), l2.float())).mean()\n",
    "            cos_similarities.append(cos_mean)\n",
    "        else:\n",
    "            logging.info(\"Layer {} not found in model 2\".format(layer))\n",
    "\n",
    "    if cos_similarities:    \n",
    "        cos = torch.Tensor(cos_similarities)\n",
    "        avg_cos = torch.mean(cos)\n",
    "        relu_cos = torch.nn.functional.relu(avg_cos)  # relu to avoid negative values\n",
    "        return relu_cos.item() if similarity else (1 - relu_cos.item())\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def euclidean_metric(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], standardized: bool = False, similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        return None\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "            if standardized:\n",
    "                l1 = (l1 - l1.mean()) / l1.std()\n",
    "                l2 = (l2 - l2.mean()) / l2.std()\n",
    "            \n",
    "            distance = torch.norm(l1 - l2, p=2)\n",
    "            if similarity:\n",
    "                norm_sum = torch.norm(l1, p=2) + torch.norm(l2, p=2)\n",
    "                similarity_score = 1 - (distance / norm_sum if norm_sum != 0 else 0)\n",
    "                distances.append(similarity_score.item())\n",
    "            else:\n",
    "                distances.append(distance.item())\n",
    "\n",
    "    if distances:\n",
    "        avg_distance = torch.mean(torch.tensor(distances))\n",
    "        return avg_distance.item()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def minkowski_metric(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], p: int, similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        return None\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "\n",
    "            distance = torch.norm(l1 - l2, p=p)\n",
    "            if similarity:\n",
    "                norm_sum = torch.norm(l1, p=p) + torch.norm(l2, p=p)\n",
    "                similarity_score = 1 - (distance / norm_sum if norm_sum != 0 else 0)\n",
    "                distances.append(similarity_score.item())\n",
    "            else:\n",
    "                distances.append(distance.item())\n",
    "\n",
    "    if distances:\n",
    "        avg_distance = torch.mean(torch.tensor(distances))\n",
    "        return avg_distance.item()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def chebyshev_metric(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        return None\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "\n",
    "            distance = torch.norm(l1 - l2, p=float('inf'))\n",
    "            if similarity:\n",
    "                norm_sum = torch.norm(l1, p=float('inf')) + torch.norm(l2, p=float('inf'))\n",
    "                similarity_score = 1 - (distance / norm_sum if norm_sum != 0 else 0)\n",
    "                distances.append(similarity_score.item())\n",
    "            else:\n",
    "                distances.append(distance.item())\n",
    "\n",
    "    if distances:\n",
    "        avg_distance = torch.mean(torch.tensor(distances))\n",
    "        return avg_distance.item()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def manhattan_metric(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        return None\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "\n",
    "            distance = torch.norm(l1 - l2, p=1)\n",
    "            if similarity:\n",
    "                norm_sum = torch.norm(l1, p=1) + torch.norm(l2, p=1)\n",
    "                similarity_score = 1 - (distance / norm_sum if norm_sum != 0 else 0)\n",
    "                distances.append(similarity_score.item())\n",
    "            else:\n",
    "                distances.append(distance.item())\n",
    "\n",
    "    if distances:\n",
    "        avg_distance = torch.mean(torch.tensor(distances))\n",
    "        return avg_distance.item()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def pearson_correlation_metric(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        return None\n",
    "\n",
    "    correlations = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "\n",
    "            if l1.shape != l2.shape:\n",
    "                min_len = min(l1.shape[0], l2.shape[0])\n",
    "                l1, l2 = l1[:min_len], l2[:min_len]\n",
    "\n",
    "            correlation = torch.corrcoef(torch.stack((l1, l2)))[0, 1]\n",
    "            if similarity:\n",
    "                adjusted_similarity = (correlation + 1) / 2\n",
    "                correlations.append(adjusted_similarity.item())\n",
    "            else:\n",
    "                correlations.append(1 - (correlation + 1) / 2)\n",
    "\n",
    "    if correlations:\n",
    "        avg_correlation = torch.mean(torch.tensor(correlations))\n",
    "        return avg_correlation.item()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repution(repution_func, model, current_round_nei_models):\n",
    "    nei_repution_score = {}\n",
    "    for nei in current_round_nei_models:\n",
    "        nei_repution_score[nei] = repution_func(model, current_round_nei_models[nei])\n",
    "    return nei_repution_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
