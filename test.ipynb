{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg(models:list):\n",
    "    return np.mean(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def krum(models:list):\n",
    "    return np.max(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(func, data):\n",
    "    return func(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = fedavg\n",
    "aggregator(a, [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.666666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregator(fedavg, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregator(krum, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(1)==int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import generate_attack_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attacked_node_list_[8, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'attack_type': 'label flipping',\n",
       "  'targeted': False,\n",
       "  'noise_injected_ratio': 20,\n",
       "  'poisoned_sample_ratio': 100},\n",
       " 1: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 2: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 3: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 4: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 5: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 6: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 7: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None},\n",
       " 8: {'attack_type': 'label flipping',\n",
       "  'targeted': False,\n",
       "  'noise_injected_ratio': 20,\n",
       "  'poisoned_sample_ratio': 100},\n",
       " 9: {'attack_type': 'no attack',\n",
       "  'targeted': None,\n",
       "  'noise_injected_ratio': None,\n",
       "  'poisoned_sample_ratio': None}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_list = list(range(10))\n",
    "attack_type = 'label flipping'\n",
    "targeted = False\n",
    "poisoned_node_ratio = 20\n",
    "noise_injected_ratio = 20\n",
    "poisoned_sample_ratio = 100\n",
    "generate_attack_matrix(node_list, attack_type, targeted, poisoned_node_ratio, noise_injected_ratio, poisoned_sample_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import OrderedDict, List\n",
    "import torch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_metric2(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        logging.info(\"Cosine similarity cannot be computed due to missing model\")\n",
    "        return None\n",
    "\n",
    "    cos_similarities = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "            if l1.shape != l2.shape:\n",
    "                # Adjust the shape of the smaller layer to match the larger layer\n",
    "                min_len = min(l1.shape[0], l2.shape[0])\n",
    "                l1, l2 = l1[:min_len], l2[:min_len]\n",
    "\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(l1.unsqueeze(0), l2.unsqueeze(0), dim=1)\n",
    "            cos_similarities.append(cos_sim.item())\n",
    "\n",
    "    if cos_similarities:\n",
    "        avg_cos_sim = torch.mean(torch.tensor(cos_similarities))\n",
    "        # result = torch.clamp(avg_cos_sim, min=0).item()\n",
    "        # return result\n",
    "        return avg_cos_sim.item() if similarity else (1 - avg_cos_sim.item())\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def cosine_metric(model1: OrderedDict, model2: OrderedDict, similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        logging.info(\"Cosine similarity cannot be computed due to missing model\")\n",
    "        return None\n",
    "\n",
    "    cos_similarities: List = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].to('cpu')\n",
    "            l2 = model2[layer].to('cpu')\n",
    "            if l1.shape != l2.shape:\n",
    "                # Adjust the shape of the smaller layer to match the larger layer\n",
    "                min_len = min(l1.shape[0], l2.shape[0])\n",
    "                l1, l2 = l1[:min_len], l2[:min_len]\n",
    "            cos = torch.nn.CosineSimilarity(dim=l1.dim() - 1)\n",
    "            cos_mean = torch.mean(cos(l1.float(), l2.float())).mean()\n",
    "            cos_similarities.append(cos_mean)\n",
    "        else:\n",
    "            logging.info(\"Layer {} not found in model 2\".format(layer))\n",
    "\n",
    "    if cos_similarities:    \n",
    "        cos = torch.Tensor(cos_similarities)\n",
    "        avg_cos = torch.mean(cos)\n",
    "        relu_cos = torch.nn.functional.relu(avg_cos)  # relu to avoid negative values\n",
    "        return relu_cos.item() if similarity else (1 - relu_cos.item())\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def euclidean_metric(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], standardized: bool = False, similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        return None\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "            if standardized:\n",
    "                l1 = (l1 - l1.mean()) / l1.std()\n",
    "                l2 = (l2 - l2.mean()) / l2.std()\n",
    "            \n",
    "            distance = torch.norm(l1 - l2, p=2)\n",
    "            if similarity:\n",
    "                norm_sum = torch.norm(l1, p=2) + torch.norm(l2, p=2)\n",
    "                similarity_score = 1 - (distance / norm_sum if norm_sum != 0 else 0)\n",
    "                distances.append(similarity_score.item())\n",
    "            else:\n",
    "                distances.append(distance.item())\n",
    "\n",
    "    if distances:\n",
    "        avg_distance = torch.mean(torch.tensor(distances))\n",
    "        return avg_distance.item()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def minkowski_metric(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], p: int, similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        return None\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "\n",
    "            distance = torch.norm(l1 - l2, p=p)\n",
    "            if similarity:\n",
    "                norm_sum = torch.norm(l1, p=p) + torch.norm(l2, p=p)\n",
    "                similarity_score = 1 - (distance / norm_sum if norm_sum != 0 else 0)\n",
    "                distances.append(similarity_score.item())\n",
    "            else:\n",
    "                distances.append(distance.item())\n",
    "\n",
    "    if distances:\n",
    "        avg_distance = torch.mean(torch.tensor(distances))\n",
    "        return avg_distance.item()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def chebyshev_metric(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        return None\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "\n",
    "            distance = torch.norm(l1 - l2, p=float('inf'))\n",
    "            if similarity:\n",
    "                norm_sum = torch.norm(l1, p=float('inf')) + torch.norm(l2, p=float('inf'))\n",
    "                similarity_score = 1 - (distance / norm_sum if norm_sum != 0 else 0)\n",
    "                distances.append(similarity_score.item())\n",
    "            else:\n",
    "                distances.append(distance.item())\n",
    "\n",
    "    if distances:\n",
    "        avg_distance = torch.mean(torch.tensor(distances))\n",
    "        return avg_distance.item()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def manhattan_metric(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        return None\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "\n",
    "            distance = torch.norm(l1 - l2, p=1)\n",
    "            if similarity:\n",
    "                norm_sum = torch.norm(l1, p=1) + torch.norm(l2, p=1)\n",
    "                similarity_score = 1 - (distance / norm_sum if norm_sum != 0 else 0)\n",
    "                distances.append(similarity_score.item())\n",
    "            else:\n",
    "                distances.append(distance.item())\n",
    "\n",
    "    if distances:\n",
    "        avg_distance = torch.mean(torch.tensor(distances))\n",
    "        return avg_distance.item()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def pearson_correlation_metric(model1: OrderedDict[str, torch.Tensor], model2: OrderedDict[str, torch.Tensor], similarity: bool = False) -> Optional[float]:\n",
    "    if model1 is None or model2 is None:\n",
    "        return None\n",
    "\n",
    "    correlations = []\n",
    "\n",
    "    for layer in model1:\n",
    "        if layer in model2:\n",
    "            l1 = model1[layer].flatten()\n",
    "            l2 = model2[layer].flatten()\n",
    "\n",
    "            if l1.shape != l2.shape:\n",
    "                min_len = min(l1.shape[0], l2.shape[0])\n",
    "                l1, l2 = l1[:min_len], l2[:min_len]\n",
    "\n",
    "            correlation = torch.corrcoef(torch.stack((l1, l2)))[0, 1]\n",
    "            if similarity:\n",
    "                adjusted_similarity = (correlation + 1) / 2\n",
    "                correlations.append(adjusted_similarity.item())\n",
    "            else:\n",
    "                correlations.append(1 - (correlation + 1) / 2)\n",
    "\n",
    "    if correlations:\n",
    "        avg_correlation = torch.mean(torch.tensor(correlations))\n",
    "        return avg_correlation.item()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repution(repution_func, model, current_round_nei_models):\n",
    "    nei_repution_score = {}\n",
    "    for nei in current_round_nei_models:\n",
    "        nei_repution_score[nei] = repution_func(model, current_round_nei_models[nei])\n",
    "    return nei_repution_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
